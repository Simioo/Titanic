{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "raw_data = pd.read_csv('../data/raw/Titanic_dataset.csv', delimiter=',', names=['Name', 'PClass', 'Age', 'Sex', 'Survived'])\n",
    "processed_data = passenger_data = pd.read_csv('../data/processed/processed_dataset.csv', delimiter=',', names=['Name', 'PClass', 'Age', 'Sex', 'Survived' , 'Family', 'Title'])\n",
    "raw_data = raw_data[1:]\n",
    "processed_data = processed_data[1:]\n",
    "\n",
    "x = processed_data[['PClass', 'Age', 'Sex', 'Family', 'Title']]\n",
    "y = processed_data['Survived']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting whether a random passenger survived the Titanic accident or not is obviously a binary classification problem. We have multiple options when it comes to choosing a model for this classification problem. We will explore some of them and compare the classification accuracies on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will try logistic regression. It's a commonly used algorithm for classification tasks. It's a good starting point to see if it matches our needs. We will use GridSearchCV to find approximate value for hyperparameter C, which represents inverse of regularization strength. Also, we will determine which norm we should use for penalization. Cross-validation will be used for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.359981134910389, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "c_range = np.random.normal(5, 1.5, 20).astype(float)\n",
    "hyperparameters = {'penalty': ['l1', 'l2'], 'C': c_range}\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(solver='liblinear'), hyperparameters, cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at previous results, we will decide to search for optimal value of hyperparameter C close to value C=1. Now we will use RandomizedSearchCV to find optimal value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 0.7140979262024134}\n"
     ]
    }
   ],
   "source": [
    "c_range = np.random.normal(1, 0.2, 20).astype(float)\n",
    "hyperparameters = {'penalty': ['l1', 'l2'], 'C': c_range}\n",
    "\n",
    "clf = RandomizedSearchCV(LogisticRegression(solver='liblinear'), param_distributions=hyperparameters, cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use L2 penalizer and value C = 1.383. The performance of this model will be measured on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89       175\n",
      "           1       0.92      0.56      0.70        86\n",
      "\n",
      "    accuracy                           0.84       261\n",
      "   macro avg       0.87      0.77      0.79       261\n",
      "weighted avg       0.85      0.84      0.83       261\n",
      "\n",
      "Accuracy of logistic regression classifier on test set: 0.84\n",
      "Accuracy of logistic regression classifier on training set: 0.81\n"
     ]
    }
   ],
   "source": [
    "best_penalty = clf.best_params_['penalty']\n",
    "best_c = clf.best_params_['C']\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', penalty=best_penalty, C=best_c)\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "print(metrics.classification_report(y_test, logreg.predict(x_test)))\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(x_test, y_test)))\n",
    "print('Accuracy of logistic regression classifier on training set: {:.2f}'.format(logreg.score(x_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Satisfying results are obtained with 84% accuracy on the test set. It's interesting to note that low recall value for class 1 (survived) is expected result. That's because of imbalanced dataset due to the fact that majority of people didn't survive the accident. It's no wonder that for some passengers who actually survived our model predicts the opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will try k-nearest neighbors algorithm. The most important question is which value of k should we pick. Using GridSearchCV, we will try many values for k and pick the best one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k value:  8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "k_range = list(range(1, 31))\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "\n",
    "classifier = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, scoring='accuracy')\n",
    "classifier.fit(x_train, y_train)\n",
    "best_n_neighbors = classifier.best_params_['n_neighbors']\n",
    "\n",
    "print('Optimal k value: ', best_n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best classifier is obtained for value k=8, so we will use that value for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.88       175\n",
      "           1       0.85      0.59      0.70        86\n",
      "\n",
      "    accuracy                           0.83       261\n",
      "   macro avg       0.84      0.77      0.79       261\n",
      "weighted avg       0.83      0.83      0.82       261\n",
      "\n",
      "Accuracy on the test set:  83.14 %\n",
      "Accuracy on the training set:  82.32 %\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(x_test)\n",
    "train_pred = knn.predict(x_train)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy on the test set: ', round(accuracy_score(y_test, y_pred)*100,2), '%')\n",
    "print('Accuracy on the training set: ', round(accuracy_score(y_train, train_pred)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got similar results to logistic regression model results. It's hard to tell which model is better for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next reasonable thing to try is Support Vector Machine. Just like in previous models, we will begin with GridSearchCV and try to find approximately good hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10.164840646763835, 'gamma': 2.5865611072692305, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "np.random.seed(123)\n",
    "g_range = np.random.uniform(7, 2.5, 10).astype(float)\n",
    "c_range = np.random.normal(7, 2.5, 10).astype(float)\n",
    "\n",
    "hyperparameters = {'kernel': ['linear', 'rbf', 'sigmoid'], 'C': c_range, 'gamma': g_range}\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(), hyperparameters, cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "best_kernel = clf.best_params_['kernel']\n",
    "best_gamma = clf.best_params_['gamma']\n",
    "best_c = clf.best_params_['C']\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use previously calculated values as base values for RandomizedSearchCV to further tune these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'gamma': 1.870493463109165, 'C': 10.656035826692136}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "g_range = np.random.uniform(2.5, 0.3, 10).astype(float)\n",
    "c_range = np.random.normal(10, 0.3, 10).astype(float)\n",
    "\n",
    "hyperparameters = {'kernel': ['linear', 'rbf', 'sigmoid'], 'C': c_range, 'gamma': g_range}\n",
    "\n",
    "clf = RandomizedSearchCV(svm.SVC(), param_distributions=hyperparameters, cv=5)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "best_kernel = clf.best_params_['kernel']\n",
    "best_gamma = clf.best_params_['gamma']\n",
    "best_c = clf.best_params_['C']\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using these values, we can get model with tuned parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89       175\n",
      "           1       0.86      0.59      0.70        86\n",
      "\n",
      "    accuracy                           0.84       261\n",
      "   macro avg       0.85      0.77      0.79       261\n",
      "weighted avg       0.84      0.84      0.83       261\n",
      "\n",
      "Accuracy on the test set:  83.52 %\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel = best_kernel, gamma=best_gamma, C=best_c, probability=True)\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy on the test set: ', round(accuracy_score(y_test, y_pred)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use models mentioned above and combine them and observe the results. Voting classifier will be used for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  83.0 %\n",
      "Accuracy on test set:  83.52 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89       175\n",
      "           1       0.92      0.56      0.70        86\n",
      "\n",
      "    accuracy                           0.84       261\n",
      "   macro avg       0.87      0.77      0.79       261\n",
      "weighted avg       0.85      0.84      0.83       261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=[('lr', logreg), ('svc', svc), ('knn', knn)], voting='soft')\n",
    "voting_classifier.fit(x_train, y_train)\n",
    "\n",
    "print('Accuracy on training set: ', round(accuracy_score(y_train, voting_classifier.predict(x_train)) * 100, 2), '%')\n",
    "print('Accuracy on test set: ', round(accuracy_score(y_test, voting_classifier.predict(x_test)) * 100, 2), '%')\n",
    "print(metrics.classification_report(y_test, logreg.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get similar results as for previous models. Similarity of predictions of different models is probably the consequence of simple dataset with small number of available features. We didn't get much from model ensemble, very likely because all models have similar predictions for the test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
